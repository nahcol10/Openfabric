# ğŸš€ Openfabric AI Developer Challenge â€“ Creative Partner

Welcome to **Creative Partner**, an intelligent AI pipeline that brings your imagination to life. This system provides a complete text-to-3D generation pipeline with advanced memory management and session handling.

## âœ¨ What This System Does

âœ… **Prompt Enhancement**: Uses local LLM (`llama3.2`) to optimize user prompts for better 3D generation  
âœ… **Image Generation**: Converts enhanced text to high-quality images via Openfabric Text-to-Image app  
âœ… **3D Model Creation**: Transforms images into detailed 3D models (.glb format) with preview videos  
âœ… **Smart Memory**: Maintains both short-term (Redis) and long-term (FAISS) conversation memory  
âœ… **Session Management**: User-specific sessions with 30-minute timeout and activity tracking  
âœ… **Multiple Interfaces**: REST API + Interactive Gradio UI  

---

## ğŸ—ï¸ System Architecture

```
User Input (REST API / Gradio UI)
 â†“
Session Manager
 â”œâ”€â”€ Creates user-specific sessions (username_sessionid)
 â”œâ”€â”€ 30-minute timeout with activity tracking
 â””â”€â”€ Memory integration per session
 â†“
Memory Manager
 â”œâ”€â”€ Short-term: Redis chat history (30min TTL)
 â”œâ”€â”€ Long-term: FAISS vector embeddings
 â””â”€â”€ Context retrieval for conversational AI
 â†“
LLM Prompt Enhancement
 â”œâ”€â”€ Model: llama3.2 via Ollama
 â”œâ”€â”€ System prompt optimized for 3D generation
 â””â”€â”€ 60-word limit for enhanced prompts
 â†“
Text-to-Image Generation
 â”œâ”€â”€ App: c25dcd829d134ea98f5ae4dd311d13bc.node3.openfabric.network
 â”œâ”€â”€ Input: Enhanced text prompt
 â””â”€â”€ Output: High-quality image (PNG)
 â†“
Image-to-3D Conversion
 â”œâ”€â”€ App: 5891a64fe34041d98b0262bb1175ff07.node3.openfabric.network
 â”œâ”€â”€ Input: Base64-encoded image
 â””â”€â”€ Output: 3D model (.glb) + Preview video (.mp4)
 â†“
File Management
 â”œâ”€â”€ Images: output_{session_id}.png
 â”œâ”€â”€ 3D Models: output_{session_id}.glb
 â”œâ”€â”€ Videos: preview_{session_id}.mp4
 â””â”€â”€ Storage: app/output_3d_model/
```

---

## ğŸ“¦ Technology Stack

| **Component** | **Technology** | **Purpose** |
|---------------|----------------|-------------|
| **Backend Framework** | Openfabric SDK 0.3.0 | Main application framework |
| **LLM** | llama3.2 (Ollama) | Prompt enhancement & conversation |
| **Short-term Memory** | Redis | Session-based chat history (30min TTL) |
| **Long-term Memory** | FAISS + SentenceTransformers | Vector embeddings for conversation recall |
| **Embeddings** | all-MiniLM-L6-v2 | Lightweight CPU-based text embeddings |
| **Text-to-Image** | Openfabric App | Professional image generation |
| **Image-to-3D** | Openfabric App | 3D model creation from images |
| **Session Management** | Custom Python | User session tracking & timeout |
| **UI Interface** | Gradio 4.x | Interactive web interface |
| **API Documentation** | Swagger UI | REST API documentation |
| **Container** | Docker | Deployment & isolation |
| **Date Parsing** | dateparser | Natural language date extraction |

---

## ğŸš€ Getting Started

### Prerequisites
- Docker installed
- Redis server running (for memory)
- Ollama with llama3.2 model (for LLM)

### Quick Start

1. **Start the main application:**
```bash
cd app/
python ignite.py
```
Access Swagger UI: `http://localhost:8888/swagger-ui`

2. **Launch Gradio interface:**
```bash
python chatbot_ui.py
```
Access UI: `http://localhost:7860`

3. **Using Docker:**
```bash
docker build -t creative-partner .
docker run -p 8888:8888 creative-partner
```

---

## ğŸ¯ API Usage

### REST API Endpoint
**POST** `/execution`

**Request:**
```json
{
  "prompt": "A magical forest with glowing mushrooms",
  "username": "john_doe",
  "attachments": []
}
```

**Response:**
```json
{
  "message": "Successfully generated 3D model from prompt (Session: john_doe_a1b2c3d4)"
}
```

### Generated Files
- **Image**: `app/output_3d_model/output_john_doe_a1b2c3d4.png`
- **3D Model**: `app/output_3d_model/output_john_doe_a1b2c3d4.glb`
- **Preview Video**: `app/output_3d_model/preview_john_doe_a1b2c3d4.mp4`

---

## ğŸ§  Memory System

### Short-term Memory (Redis)
- **Storage**: Session-based chat history
- **TTL**: 30 minutes per session
- **Fallback**: In-memory storage if Redis unavailable
- **Format**: LangChain message history

### Long-term Memory (FAISS)
- **Storage**: Vector embeddings of conversations
- **Search**: Semantic similarity search
- **Metadata**: Date, session_id, user_id, type
- **Persistence**: In-memory with optional file save

### Memory Integration
```python
# Context retrieval example
context_msgs = session.memory.fetch_context(user_prompt)
# 1. Check recent Redis messages
# 2. Fallback to FAISS vector search
# 3. Return combined context for LLM
```

---

## ğŸ“ Project Structure

```
app/
â”œâ”€â”€ main.py                 # Main application logic
â”œâ”€â”€ ignite.py              # Application startup
â”œâ”€â”€ session_manager.py     # Session handling
â”œâ”€â”€ chatbot_ui.py         # Gradio interface
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ Dockerfile            # Container configuration
â”œâ”€â”€ start.sh              # Startup script
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ stub.py           # Openfabric app client
â”‚   â””â”€â”€ remote.py         # WebSocket proxy client
â”œâ”€â”€ memory/
â”‚   â”œâ”€â”€ __init__.py       # System prompts
â”‚   â”œâ”€â”€ short_term_memory.py  # Redis integration
â”‚   â”œâ”€â”€ long_term_memory.py   # FAISS vector store
â”‚   â””â”€â”€ memory_manager.py     # Memory coordination
â”œâ”€â”€ ontology_*/           # Auto-generated schemas
â”œâ”€â”€ config/               # Application configuration
â”œâ”€â”€ datastore/            # Execution logs & state
â””â”€â”€ output_3d_model/      # Generated files
```

---

## ğŸ”§ Configuration

### App Configuration (`config/state.json`)
```json
{
  "super-user": {
    "app_ids": [
      "c25dcd829d134ea98f5ae4dd311d13bc.node3.openfabric.network",
      "5891a64fe34041d98b0262bb1175ff07.node3.openfabric.network"
    ]
  }
}
```

### Environment Variables
```bash
REDIS_URL=redis://localhost:6379/0
OLLAMA_URL=http://localhost:11434
```

---

## ğŸ“Š System Features

### Session Management
- **User-specific sessions**: `username_sessionid` format
- **Activity tracking**: Last activity timestamp
- **Timeout handling**: 30-minute automatic cleanup
- **Memory preservation**: Stores session summary before cleanup

### Error Handling
- **Redis fallback**: In-memory chat history when Redis unavailable
- **Connection resilience**: Handles Openfabric app failures gracefully
- **Logging**: Comprehensive error tracking in datastore/

### File Management
- **Session-based naming**: Prevents file conflicts
- **Multiple formats**: PNG images, GLB models, MP4 previews
- **Automatic cleanup**: Old files managed by session lifecycle

---

## ğŸ¨ Example Outputs

**Input:** `"cat"`
**Session:** `lochan_8413bd70`
**Generated Files:**
- `output_lochan_8413bd70.png` - Enhanced cat image
- `output_lochan_8413bd70.glb` - 3D cat model

**Successful Response:**
```json
{"message": "Successfully generated 3D model from prompt (Session: lochan_8413bd70)"}
```

---

## ğŸ” Monitoring & Debugging

### Execution Logs
- **Location**: `app/datastore/executions/`
- **Contains**: Input prompts, outputs, error traces
- **Format**: JSON with execution metadata

### Memory Logs
- **Short-term**: `app/datastore/short_term_log.jsonl`
- **Long-term**: In-memory FAISS store
- **Session tracking**: User activity and memory summary

---

## âš¡ Performance & Scalability

### Memory Efficiency
- **FAISS**: CPU-optimized vector operations
- **Redis TTL**: Automatic memory cleanup
- **Lightweight embeddings**: all-MiniLM-L6-v2 model

### Session Isolation
- **User separation**: Individual session namespaces
- **File organization**: Session-based file naming
- **Memory segmentation**: Per-user memory contexts

---

## ğŸ› ï¸ Development & Customization

### Adding New Openfabric Apps
1. Update `config/state.json` with new app IDs
2. Modify `core/stub.py` for new app schemas
3. Update `main.py` execution pipeline

### Custom Memory Strategies
- **Short-term**: Modify `memory/short_term_memory.py`
- **Long-term**: Customize `memory/long_term_memory.py`
- **Integration**: Update `memory/memory_manager.py`

### Prompt Engineering
- **System prompt**: `memory/__init__.py`
- **Enhancement logic**: LLM chain in `short_term_memory.py`
- **Word limits**: 60-word optimization for 3D generation

---

## ğŸ“œ License

This project is licensed under the **MIT License**.

---

## ğŸ™ Acknowledgments

- **[Openfabric AI](https://openfabric.network/)** - AI application platform
- **[Meta LLaMA](https://llama.meta.com/)** - Language model technology
- **[FAISS](https://github.com/facebookresearch/faiss)** - Vector similarity search
- **[LangChain](https://langchain.com/)** - LLM framework integration
- **[Gradio](https://gradio.app/)** - Interactive web interfaces

---

## ğŸ¯ Future Enhancements

- [ ] Voice input integration
- [ ] Multiple 3D export formats
- [ ] Advanced prompt templates
- [ ] Batch processing capabilities
- [ ] Real-time collaboration features
- [ ] Custom model fine-tuning

---

**Creative Partner isn't just a toolâ€”it's your AI-powered creative companion that remembers, learns, and grows with your imagination.** ğŸ¨âœ¨
